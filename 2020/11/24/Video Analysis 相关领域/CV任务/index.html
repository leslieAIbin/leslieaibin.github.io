<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Video Analysis 相关领域 | Mr.Ai</title><meta name="keywords" content="Video Analysis"><meta name="author" content="Leslie"><meta name="copyright" content="Leslie"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="Action Recognition(行为识别)也可以叫  Action Classification  任务目的给一个视频片段进行分类，类别通常是各类人的动作 任务特点简化了问题，一般使用的数据库都先将动作分割好了，一个视频片断中包含一段明确的动作，时间较短（几秒钟）且有唯一确定的label。所以也可以看作是输入为视频，输出为动作标签的多分类问题。此外，动作识别数据库中的动作一般都比较明确，周围">
<meta property="og:type" content="article">
<meta property="og:title" content="Video Analysis 相关领域">
<meta property="og:url" content="https://leslieaibin.github.io/2020/11/24/Video%20Analysis%20%E7%9B%B8%E5%85%B3%E9%A2%86%E5%9F%9F/CV%E4%BB%BB%E5%8A%A1/index.html">
<meta property="og:site_name" content="Mr.Ai">
<meta property="og:description" content="Action Recognition(行为识别)也可以叫  Action Classification  任务目的给一个视频片段进行分类，类别通常是各类人的动作 任务特点简化了问题，一般使用的数据库都先将动作分割好了，一个视频片断中包含一段明确的动作，时间较短（几秒钟）且有唯一确定的label。所以也可以看作是输入为视频，输出为动作标签的多分类问题。此外，动作识别数据库中的动作一般都比较明确，周围">
<meta property="og:locale">
<meta property="og:image" content="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/24.jpg?raw=true">
<meta property="article:published_time" content="2020-11-23T16:15:42.000Z">
<meta property="article:modified_time" content="2020-11-24T07:46:05.299Z">
<meta property="article:author" content="Leslie">
<meta property="article:tag" content="Video Analysis">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/24.jpg?raw=true"><link rel="shortcut icon" href="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/01.jpg?raw=true"><link rel="canonical" href="https://leslieaibin.github.io/2020/11/24/Video%20Analysis%20%E7%9B%B8%E5%85%B3%E9%A2%86%E5%9F%9F/CV%E4%BB%BB%E5%8A%A1/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2020-11-24 15:46:05'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Mr.Ai" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/01.jpg?raw=true" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">37</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fas fa-pen"></i><span> 自言自语</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/24.jpg?raw=true)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Mr.Ai</a></span><span id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fas fa-pen"></i><span> 自言自语</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><h1 class="post-title">Video Analysis 相关领域</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-11-23T16:15:42.000Z" title="Created 2020-11-24 00:15:42">2020-11-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2020-11-24T07:46:05.299Z" title="Updated 2020-11-24 15:46:05">2020-11-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Video-Analysis/">Video Analysis</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Action-Recognition-行为识别"><a href="#Action-Recognition-行为识别" class="headerlink" title="Action Recognition(行为识别)"></a>Action Recognition(行为识别)</h1><p>也可以叫  <strong>Action Classification</strong> </p>
<h2 id="任务目的"><a href="#任务目的" class="headerlink" title="任务目的"></a>任务目的</h2><p>给一个视频片段进行分类，类别通常是各类人的动作</p>
<h2 id="任务特点"><a href="#任务特点" class="headerlink" title="任务特点"></a>任务特点</h2><p>简化了问题，一般使用的数据库都先将动作分割好了，一个视频片断中包含一段明确的动作，时间较短（几秒钟）且有唯一确定的label。所以也可以看作是输入为视频，输出为动作标签的多分类问题。此外，动作识别数据库中的动作一般都比较明确，周围的干扰也相对较少（不那么real-world）。有点像图像分析中的Image Classification任务。</p>
<h2 id="难点-关键点"><a href="#难点-关键点" class="headerlink" title="难点/关键点"></a>难点/关键点</h2><ul>
<li>强有力的特征：即如何在视频中提取出能更好的描述视频判断的特征。特征越强，模型的效果通常较好。</li>
<li>特征的编码（encode）/融合（fusion）：这一部分包括两个方面，第一个方面是非时序的，在使用多种特征的时候如何编码/融合这些特征以获得更好的效果；另外一个方面是时序上的，由于视频很重要的一个特性就是其时序信息，一些动作看单帧的图像是无法判断的，只能通过时序上的变化判断，所以需要将时序上的特征进行编码或者融合，获得对于视频整体的描述。</li>
<li>算法速度：虽然在发论文刷数据库的时候算法的速度并不是第一位的。但高效的算法更有可能应用到实际场景中去。</li>
</ul>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>行为识别的数据库比较多，这里主要介绍两个最常用的数据库，也是近年这个方向的论文必做的数据库。</p>
<ul>
<li>UCF101:来源为YouTube视频，共计101类动作，13320段视频。共有5个大类的动作：1)人-物交互；2)肢体运动；3)人-人交互；4)弹奏乐器；5)运动。数据库主页为：<a target="_blank" rel="noopener" href="https://www.crcv.ucf.edu/data/UCF101.php">UCF101</a>。文章的题图为UCF各类视频的示意图。</li>
<li>HMDB51:来源为YouTube视频，共计51类动作，约7000段视频。数据库主页为：<a target="_blank" rel="noopener" href="https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">HMDB51</a></li>
</ul>
<p>在Actioin Recognition中，实际上还有一类骨架数据库，比如MSR Action 3D，HDM05，SBU Kinect Interaction Dataset等。这些数据库已经提取了每帧视频中人的骨架信息，基于骨架信息判断运动类型。不做详细介绍</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><a target="_blank" rel="noopener" href="https://github.com/jinwchoi/awesome-action-recognition">Action Recognition方法</a></p>
<p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/image-20201124105621642.png" alt="image-20201124105621642"></p>
<h1 id="Temporal-Action-Proposals（时间行动建议）"><a href="#Temporal-Action-Proposals（时间行动建议）" class="headerlink" title="Temporal Action Proposals（时间行动建议）"></a>Temporal Action Proposals（时间行动建议）</h1><h2 id="任务目的-1"><a href="#任务目的-1" class="headerlink" title="任务目的"></a>任务目的</h2><p>Temporal Action Proposal任务不需要对活动分类，只需要找出proposals,主要目的是将长视频根据语义分割成多个segment。所以判断找的temporal proposals全不全就可以测评方法好坏，常用average recall (AR) ，Average Recall vs. Average Number of Proposals per Video (AR-AN) 即曲线下的面积(ActivityNet Challenge 2017就用这个测评此项任务)。如下图：</p>
<p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/uniform_random_proposal_performance.png" alt="img"></p>
<h2 id="任务特点-1"><a href="#任务特点-1" class="headerlink" title="任务特点"></a>任务特点</h2><p>为了适应视频数据集的特点，好的action proposal应该具有如下的特点：</p>
<ol>
<li>能够高效地表示一个视频间隔（temporal segment），计算开销小。</li>
<li>将可能具有action的视频间隔（temporal segment）找出来。</li>
<li>初步识别出视频间隔中的动作是否为我们感兴趣的动作类别之一。</li>
</ol>
<h2 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h2><p>目前使用比较广泛的数据集为ActivityNet-1.3（2016年发布）以及早些年的一个相对较小的数据集THUMOS14。</p>
<p>THUMOS14是一个包含action recognition和action localization任务的比赛，其中训练集是trimmed UCF101，所以训练集不能被用来训练action proposal网络。验证集集有1010个视频，测试集有1574个视频。但是验证集中只有200个视频有temporal label，测试集中只有212个。一般情况下，大家使用验证集来训练action proposal网络，然后在测试集上查看效果。</p>
<p>用于训练的验证集上每个类别平均有150个动作时序标注，每个动作平均的持续时间为4.04秒。一共有3007个动作时序标注，标注了的动作共持续12159.8秒。测试集上的每个类别平均有167.9个动作时序标注，每个动作平均的持续时间为4.47秒，标注了的动作共持续15040。3秒。验证集上不同类别的标注个数及平均动作持续时间如下图</p>
<p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/v2-d323e3a24d32ee79dcb862f39740f1ea_720w.jpg" alt="img"></p>
<p>ActivityNet-1.3共有200个类别，训练集有10024个视频，验证集有4926个视频，测试集有5044个视频。官方提供的视频分辨率为320x240，除了视频之外还提供了按照5FPS 使用的代码，以及使用ResNet-152在抽好的帧上提取的feature。一般来说对于Action Proposal需要在提供的帧上进行实验，或者为了更好地利用标注也可以自己进行密集抽帧。</p>
<p>就标注来看，共有23065个动作时序标注，平均每个视频有1.15个时序标注。视频的平均时长为128秒，标注的平均时长为49.2秒。</p>
<p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/v2-ab716c73c416fdfc9fe84ad3219bab28_720w.jpg" alt="img">时序动作标注长度分布图</p>
<p>就类别分布来看，平均每个类别有115个标注，平均每个类别的动作持续时间为51.3秒。</p>
<p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/v2-62bd5ac0bf060e9bed1fe6ef92cf4212_720w.jpg" alt="img">类别动作持续时间图</p>
<p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/image-20201123211330471.png" alt="image-20201123211330471"></p>
<p><strong>ActivityNet 任务的提交格式</strong></p>
<h1 id="Temporal-Action-Localization-（时序动作定位）"><a href="#Temporal-Action-Localization-（时序动作定位）" class="headerlink" title="Temporal Action Localization （时序动作定位）"></a>Temporal Action Localization （时序动作定位）</h1><p>也可以叫 <strong>Temporal Action Detection （时序行为检测）</strong> </p>
<h2 id="任务目的-2"><a href="#任务目的-2" class="headerlink" title="任务目的"></a>任务目的</h2><p>给定一段未分割的长视频，算法需要检测视频中的行为片段（action instance），包括其<strong>开始时间、结束时间以及类别</strong>。一段视频中可能包含一个或多个行为片段。</p>
<p>(1) temporal action proposal generation: 即进行时序动作提名，产生候选的视频时序片段，即相当于Faster-RCNN中的RPN网络的作用；</p>
<p>(2) action classification: 即判断候选视频时序片段的动作类别。两个部分结合在一起，即实现了视频中的时序动作检测。目前视频分类算法的精度其实以及比较高了，然而时序检测的效果依旧比较低，主要的性能瓶颈在于时序提名环节。</p>
<h2 id="任务特点-2"><a href="#任务特点-2" class="headerlink" title="任务特点"></a>任务特点</h2><ul>
<li><p><strong>特点1</strong></p>
<p><strong>action recognition</strong>与<strong>temporal action detection</strong>之间的关系同 <strong>image classfication</strong>与 <strong>object detection</strong>之间的关系非常像。基于image classification问题，发展出了许多强大的网络模型（比如ResNet,VGGNet等），这些模型在object detection的方法中起到了很大的作用。同样，action recognition的相关模型（如2stream，C3D, iDT等)也被广泛的用在temporal action detection的方法中。</p>
</li>
<li><p><strong>特点2</strong></p>
<p>由于temporal action detection和object detection之间存在一定的相似性，所以很多temporal action detection方法都采用了与一些object detection方法相似的框架（最常见的就是参考R-CNN系列方法）。具体的会在后面的论文介绍中讲到。</p>
</li>
</ul>
<h2 id="难点-关键点-1"><a href="#难点-关键点-1" class="headerlink" title="难点/关键点"></a>难点/关键点</h2><ul>
<li><p><strong>难点1</strong></p>
<p>在目标检测中，物体目标的边界通常都是非常明确的，所以可以标注出较为明确的边界框。但时序行为的边界很多时候并不是很明确，什么时候一个行为算开始，什么时候行为算结束常常无法给出一个准确的边界（指精确的第几帧）。</p>
</li>
<li><p><strong>难点2</strong></p>
<p>只使用静态图像的信息，而不结合时序信息在行为识别中是可以的（虽然结合时序信息效果会更好）。但在时序行为检测中，是无法只使用静态图像信息的。必须结合时序的信息，比如使用RNN读入每帧图像上用CNN提取的特征，或是用时序卷积等。</p>
</li>
<li><p><strong>难点3</strong></p>
<p>时序行为片段的时间跨度变化可能非常大。比如在ActivityNet中，最短的行为片段大概1s左右，最长的行为片段则超过了200s。巨大的时长跨度，也使得检测时序动作非常难。</p>
</li>
</ul>
<p><strong>任务关键点</strong></p>
<p>我认为设计一个好的时序行为检测方法的关键主要在于以下两点：</p>
<ul>
<li><p>高质量的时序片段（<strong>Proposals</strong>）：很多方法都是使用<strong>Proposal + classification</strong>的框架。对于这类方法，重要的是较高的proposal质量（即在保证平均召回率的情况下，尽可能减少proposal的数量）。此外，对于所有方法，获取准确的时序行为边界都是非常重要的。</p>
</li>
<li><p>准确的分类（<strong>Classification</strong>）：即能准确的得到时序行为片段的类别信息。这里通常都会使用行为识别中的一些方法与模型。</p>
</li>
</ul>
<h2 id="数据集-2"><a href="#数据集-2" class="headerlink" title="数据集"></a>数据集</h2><p>时序行为检测的数据库也有很多，下面主要介绍几个常用的主流数据库：</p>
<ul>
<li><strong>THUMOS 2014</strong>：该数据集即为THUMOS Challenge 2014，地址为<a target="_blank" rel="noopener" href="http://crcv.ucf.edu/THUMOS14/">THUMOS 2014</a>。该数据集包括行为识别和时序行为检测两个任务。它的训练集为UCF101数据集，包括101类动作，共计13320段分割好的视频片段。THUMOS2014的验证集和测试集则分别包括1010和1574个未分割过的视频。在时序行为检测任务中，只有20类动作的未分割视频是有时序行为片段标注的，包括200个验证集视频（包含3007个行为片段）和213个测试集视频（包含3358个行为片段）。这些经过标注的未分割视频可以被用于训练和测试时序行为检测模型。实际上之后还有THUMOS Challenge 2015,包括更多的动作类别和视频数，但由于上面可以比较的方法不是很多，所以目前看到的文章基本上还是在THUMOS14上进行实验。</li>
<li><strong>MEXaction2</strong>：MEXaction2数据集中包含两类动作：骑马和斗牛。该数据集由三个部分组成：YouTube视频，UCF101中的骑马视频以及INA视频，数据集地址为<a target="_blank" rel="noopener" href="http://mexculture.cnam.fr/xwiki/bin/view/Datasets/Mex+action+dataset">MEXaction2</a> 。其中YouTube视频片段和UCF101中的骑马视频是分割好的短视频片段，被用于训练集。而INA视频为多段长的未分割的视频，时长共计77小时，且被分为训练，验证和测试集三部分。训练集中共有1336个行为片段，验证集中有310个行为片段，测试集中有329个行为片断。且MEXaction2数据集的特点是其中的未分割视频长度都非常长，被标注的行为片段仅占视频总长的很低比例</li>
<li><strong>ActivityNet</strong>: 目前最大的数据库，同样包含分类和检测两个任务。数据集地址为<a target="_blank" rel="noopener" href="http://activity-net.org/">Activity Net</a> ，这个数据集仅提供视频的youtube链接，而不能直接下载视频，所以还需要用python中的youtube下载工具来自动下载。该数据集包含200个动作类别，20000（训练+验证+测试集）左右的视频，视频时长共计约700小时。由于这个数据集实在太大了，我的实验条件下很难完成对其的实验，所以我之前主要还是在THUMOS14和MEXaction2上进行实验。</li>
</ul>
<p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/image-20201123203536610.png" alt="image-20201123203536610"></p>
<p><strong>ActivityNet 任务的提交格式</strong></p>
<h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/image-20201124110351933.png" alt="image-20201124110351933"></p>
<h1 id="Temporal-Action-Parsing-TAP-时态动作解析"><a href="#Temporal-Action-Parsing-TAP-时态动作解析" class="headerlink" title="Temporal Action Parsing (TAP) (时态动作解析)"></a>Temporal Action Parsing (TAP) (时态动作解析)</h1><p>相关论文《Intra- and Inter-Action Understanding via Temporal Action Parsing》</p>
<h2 id="相关数据集Datasets"><a href="#相关数据集Datasets" class="headerlink" title="相关数据集Datasets"></a>相关数据集Datasets</h2><ol>
<li>only class labels：<ul>
<li>KTH, Weizmann, UCFSports, Olympic</li>
<li>UCF101, HMDB51, Sports1M, Kinetics</li>
</ul>
</li>
<li>boundaries of actions in untrimmed video:<ul>
<li>THUMOS’15, ActivityNet, Charades, HACS, AVA</li>
</ul>
</li>
<li>finegrained annotations for action instances(author’s): 动作实例的细粒度注释<ul>
<li>Salads, Breakfast, <strong>MPIICooking, JIGSAWS</strong></li>
</ul>
</li>
</ol>
<p>为2020年CVPR 商汤等新提出的任务，在一段动作视频中，定义一连串子动作（sub-action），动作解析即定位这些子动作的开始帧。该任务可更好的进行动作间和动作内部的视频理解。</p>
<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><ul>
<li>对动作实例的内部结构的详细了解，特别是在时间维度上（TAP/TAS）<ul>
<li>TAP仅提供了子动作之间的边界，而这些边界的监督作用却明显较弱</li>
<li>TAS的目标是在一组预定义的子动作中标记动作实例的每一个框架，这些子动作可以在一个监督下完成</li>
</ul>
</li>
</ul>
<p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/image-20201123213225627.png" alt="image-20201123213225627"></p>
<p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/image-20201124102457517.png" alt="image-20201124102457517"></p>
<p>特点：  每个未剪辑的视频类别是一样的，子动作的边界给出，但是没有给出子动作的类比</p>
<h2 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h2><p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/image-20201124104217442.png" alt="image-20201124104217442"></p>
<p>动作边界检测。我们借助于序列模型，特别是时间卷积网络(TCN)，来估计动作状态变化的出现。给定一个T帧片段，在顶部构建一个两层时间卷积网络来密集预测每个帧的标量。接下来，带注释的时间边界及其k个相邻帧被标记为1，其余的被设置为0。由于正样本(即子动作变化点)和负样本之间的不平衡，使用加权二进制交叉熵损失来优化网络。在推断期间，一旦输出超过某个阈值θc，例如0.5，就检测到子动作。</p>
<p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/image-20201124104510876.png" alt="image-20201124104510876">\</p>
<p>弱监督时间动作分割。时间动作分割旨在用一组预定义的子动作来标记动作实例的每个帧。在弱监督设置中，仅提供按发生顺序排列的子动作列表，而没有精确的时间位置。我们通过迭代软边界分配(ISBA)  [6]和连接主义时间建模(CTM) [18]选择了两个有代表性的方法。对于ISBA，我们通过提取帧级特征{ fi } N i =  1并将它们预分组为K个簇来生成伪标签。对CTM来说，最初的训练目标是最大化预定义目标标记的对数似然性。在我们的例子中，损失被改变为所有可能标签的对数似然的和，因为所有k个有区别的随机抽样子动作可能是一个可能的解决方案。在推断过程中，我们使用简单的最佳路径解码，即在每个时间戳连接最活跃的输出。</p>
<h2 id="评测"><a href="#评测" class="headerlink" title="评测"></a>评测</h2><p><img src="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/image-20201124103739413.png" alt="image-20201124103739413"></p>
<p>输出的是每个子动作的开始帧  用于评测的是召回率，精确率，f1分数</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26603387">Video Analysis</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32265681">视频分析入门之 Action Proposal</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80333569">Action Proposal &amp; Boundary系列</a></p>
<p><a target="_blank" rel="noopener" href="https://pianshen.com/article/36421969810/">TAPOS论文分享</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/75444151">边界匹配网络详解</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/39327364">用于时序动作提名生成的边界敏感网络</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Leslie</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://leslieaibin.github.io/2020/11/24/Video%20Analysis%20%E7%9B%B8%E5%85%B3%E9%A2%86%E5%9F%9F/CV%E4%BB%BB%E5%8A%A1/">https://leslieaibin.github.io/2020/11/24/Video%20Analysis%20%E7%9B%B8%E5%85%B3%E9%A2%86%E5%9F%9F/CV%E4%BB%BB%E5%8A%A1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Video-Analysis/">Video Analysis</a></div><div class="post_share"><div class="social-share" data-image="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/24.jpg?raw=true" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/11/30/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"><img class="prev-cover" src="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/25.jpg?raw=true" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">2.操作系统-进程管理</div></div></a></div><div class="next-post pull-right"><a href="/2020/11/22/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/"><img class="next-cover" src="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/23.jpg?raw=true" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">1.操作系统-操作系统概述</div></div></a></div></nav></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/01.jpg?raw=true" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Leslie</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">37</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">11</div></a></div></div><a class="button--animated" id="card-info-btn"><i class="fas fa-heart"></i><span>瓶中的水 青天的云</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/leslieAIbin" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://www.instagram.com/leslieaibin/" target="_blank" title="Ins"><i class="fab fa-instagram"></i></a><a class="social-icon" href="mailto:1915612226@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="http://test-1874253.oss-cn-beijing.aliyuncs.com/img/image-20201122143121657.png" target="_blank" title="weixin"><i class="fab fa-weixin"></i></a></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Action-Recognition-%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB"><span class="toc-number">1.</span> <span class="toc-text">Action Recognition(行为识别)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E7%9B%AE%E7%9A%84"><span class="toc-number">1.1.</span> <span class="toc-text">任务目的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E7%89%B9%E7%82%B9"><span class="toc-number">1.2.</span> <span class="toc-text">任务特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%BE%E7%82%B9-%E5%85%B3%E9%94%AE%E7%82%B9"><span class="toc-number">1.3.</span> <span class="toc-text">难点&#x2F;关键点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.4.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">1.5.</span> <span class="toc-text">方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Temporal-Action-Proposals%EF%BC%88%E6%97%B6%E9%97%B4%E8%A1%8C%E5%8A%A8%E5%BB%BA%E8%AE%AE%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">Temporal Action Proposals（时间行动建议）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E7%9B%AE%E7%9A%84-1"><span class="toc-number">2.1.</span> <span class="toc-text">任务目的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E7%89%B9%E7%82%B9-1"><span class="toc-number">2.2.</span> <span class="toc-text">任务特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86-1"><span class="toc-number">2.3.</span> <span class="toc-text">数据集</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Temporal-Action-Localization-%EF%BC%88%E6%97%B6%E5%BA%8F%E5%8A%A8%E4%BD%9C%E5%AE%9A%E4%BD%8D%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">Temporal Action Localization （时序动作定位）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E7%9B%AE%E7%9A%84-2"><span class="toc-number">3.1.</span> <span class="toc-text">任务目的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E7%89%B9%E7%82%B9-2"><span class="toc-number">3.2.</span> <span class="toc-text">任务特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%BE%E7%82%B9-%E5%85%B3%E9%94%AE%E7%82%B9-1"><span class="toc-number">3.3.</span> <span class="toc-text">难点&#x2F;关键点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86-2"><span class="toc-number">3.4.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95-1"><span class="toc-number">3.5.</span> <span class="toc-text">方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Temporal-Action-Parsing-TAP-%E6%97%B6%E6%80%81%E5%8A%A8%E4%BD%9C%E8%A7%A3%E6%9E%90"><span class="toc-number">4.</span> <span class="toc-text">Temporal Action Parsing (TAP) (时态动作解析)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E9%9B%86Datasets"><span class="toc-number">4.1.</span> <span class="toc-text">相关数据集Datasets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1"><span class="toc-number">4.2.</span> <span class="toc-text">任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Baseline"><span class="toc-number">4.3.</span> <span class="toc-text">Baseline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E6%B5%8B"><span class="toc-number">4.4.</span> <span class="toc-text">评测</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">5.</span> <span class="toc-text">参考</span></a></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/05/17/JVM/14.StringTable/" title="14.JVM —— StringTable"><img src="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/6.jpg?raw=true" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="14.JVM —— StringTable"/></a><div class="content"><a class="title" href="/2021/05/17/JVM/14.StringTable/" title="14.JVM —— StringTable">14.JVM —— StringTable</a><time datetime="2021-05-17T01:15:42.000Z" title="Created 2021-05-17 09:15:42">2021-05-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/15/JVM/13.%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E/" title="13.JVM —— 执行引擎"><img src="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/5.jpg?raw=true" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="13.JVM —— 执行引擎"/></a><div class="content"><a class="title" href="/2021/05/15/JVM/13.%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E/" title="13.JVM —— 执行引擎">13.JVM —— 执行引擎</a><time datetime="2021-05-15T11:15:42.000Z" title="Created 2021-05-15 19:15:42">2021-05-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/15/JVM/12.%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98Direct%20Memory/" title="12.JVM —— 直接内存"><img src="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/4.jpg?raw=true" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="12.JVM —— 直接内存"/></a><div class="content"><a class="title" href="/2021/05/15/JVM/12.%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98Direct%20Memory/" title="12.JVM —— 直接内存">12.JVM —— 直接内存</a><time datetime="2021-05-15T01:15:42.000Z" title="Created 2021-05-15 09:15:42">2021-05-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/14/JVM/11.%E5%AF%B9%E8%B1%A1%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80%E4%B8%8E%E8%AE%BF%E9%97%AE%E5%AE%9A%E4%BD%8D/" title="11.JVM —— 对象实例化内存布局与访问定位"><img src="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/3.jpg?raw=true" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="11.JVM —— 对象实例化内存布局与访问定位"/></a><div class="content"><a class="title" href="/2021/05/14/JVM/11.%E5%AF%B9%E8%B1%A1%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80%E4%B8%8E%E8%AE%BF%E9%97%AE%E5%AE%9A%E4%BD%8D/" title="11.JVM —— 对象实例化内存布局与访问定位">11.JVM —— 对象实例化内存布局与访问定位</a><time datetime="2021-05-14T01:15:42.000Z" title="Created 2021-05-14 09:15:42">2021-05-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/14/JVM/10.%E6%96%B9%E6%B3%95%E5%8C%BA/" title="10.JVM —— 方法区"><img src="https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/41.jpg?raw=true" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="10.JVM —— 方法区"/></a><div class="content"><a class="title" href="/2021/05/14/JVM/10.%E6%96%B9%E6%B3%95%E5%8C%BA/" title="10.JVM —— 方法区">10.JVM —— 方法区</a><time datetime="2021-05-13T21:15:42.000Z" title="Created 2021-05-14 05:15:42">2021-05-14</time></div></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://github.com/leslieAIbin/leslieaibin.github.io/blob/main/imgs/24.jpg?raw=true)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Leslie</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>